"""
Claude LLM integration module using Anthropic API.

This module provides a clean and reliable LLM integration using
Anthropic's Claude models with environment variable configuration.
"""

import os
from typing import Dict, Any, List, Optional

from dotenv import load_dotenv
from .data_analyzer import DataAnalyzer
from .utils import format_data_as_json

# Load environment variables from .env file
load_dotenv()

# Import Anthropic components
try:
    import anthropic

    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    print("‚ùå Anthropic not installed. Run: uv add anthropic")


class SimpleLLMProcessor:
    """
    Simple LLM processor using Anthropic Claude.

    This class provides clean LLM integration with Claude models.
    API key is loaded from ANTHROPIC_API_KEY environment variable.
    """

    def __init__(self, data_analyzer: DataAnalyzer):
        """
        Initialize the Claude LLM processor.

        Args:
            data_analyzer: DataAnalyzer instance with loaded data
        """
        self.analyzer = data_analyzer
        self.client: Optional[anthropic.Anthropic] = self._initialize_claude()

        # Map questions to analysis functions
        self.question_mapping = {
            "crypto_payment": self.analyzer.analyze_crypto_payment_earnings,
            "regional_income": self.analyzer.analyze_regional_income_distribution,
            "expert_projects": self.analyzer.analyze_expert_projects_completion,
            "experience_rates": self.analyzer.analyze_experience_vs_rates,
            "specialization_earnings": self.analyzer.analyze_specialization_earnings,
            "platform_performance": self.analyzer.analyze_platform_performance,
            "summary": self.analyzer.get_comprehensive_summary,
        }

    def _initialize_claude(self) -> Optional[anthropic.Anthropic]:
        """
        Initialize Claude client through Anthropic API.

        Returns:
            Anthropic client instance or None if not available
        """
        if not ANTHROPIC_AVAILABLE:
            print("‚ùå Anthropic SDK is not available")
            return None

        # Get API key from environment
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            print("‚ùå ANTHROPIC_API_KEY environment variable not set")
            print("üí° Set it with: export ANTHROPIC_API_KEY='your-api-key-here'")
            return None

        try:
            # Get configuration from environment variables with defaults
            timeout = int(os.getenv("CLAUDE_TIMEOUT", "30"))

            # Configure proxy if available using httpx
            http_client_kwargs = {}

            if (
                os.getenv("HTTP_PROXY")
                or os.getenv("HTTPS_PROXY")
                or os.getenv("SOCKS_PROXY")
            ):
                try:
                    import httpx

                    proxies = {}
                    if os.getenv("HTTP_PROXY"):
                        proxies["http://"] = os.getenv("HTTP_PROXY")
                    if os.getenv("HTTPS_PROXY"):
                        proxies["https://"] = os.getenv("HTTPS_PROXY")
                    if os.getenv("SOCKS_PROXY"):
                        proxies["http://"] = os.getenv("SOCKS_PROXY")
                        proxies["https://"] = os.getenv("SOCKS_PROXY")

                    if proxies:
                        # Create custom httpx client with proxy
                        http_client = httpx.Client(
                            proxy=proxies.get("https://") or proxies.get("http://"),
                            timeout=timeout,
                        )
                        http_client_kwargs["http_client"] = http_client
                        print(f"üåê Using proxy for Claude: {list(proxies.keys())}")

                except ImportError:
                    print("‚ö†Ô∏è httpx not available for proxy support")
                except Exception as e:
                    print(f"‚ö†Ô∏è Proxy configuration failed: {e}")

            # Initialize Anthropic client
            client = anthropic.Anthropic(
                api_key=api_key,
                timeout=timeout,
                **http_client_kwargs,  # type: ignore
            )

            # Get model name for display
            model = os.getenv("CLAUDE_MODEL", "claude-3-5-sonnet-20241022")
            print(f"‚úÖ Claude (Anthropic) initialized successfully with model {model}")
            return client

        except Exception as e:
            print(f"‚ùå Claude initialization failed: {e}")
            return None

    def classify_question(self, question: str) -> str:
        """
        Classify the user question to determine which analysis to run.

        Args:
            question: User's natural language question

        Returns:
            Analysis type key
        """
        question_lower = question.lower()

        # Enhanced keyword matching
        if any(
            word in question_lower
            for word in [
                "–∫—Ä–∏–ø—Ç–æ",
                "crypto",
                "–∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç",
                "–æ–ø–ª–∞—Ç",
                "payment",
                "bitcoin",
            ]
        ):
            return "crypto_payment"
        elif any(
            word in question_lower
            for word in [
                "—Ä–µ–≥–∏–æ–Ω",
                "region",
                "—Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è",
                "–≥–µ–æ–≥—Ä–∞—Ñ",
                "—Å—Ç—Ä–∞–Ω–∞",
                "country",
            ]
        ):
            return "regional_income"
        elif any(
            word in question_lower
            for word in ["—ç–∫—Å–ø–µ—Ä—Ç", "expert", "100", "–ø—Ä–æ–µ–∫—Ç", "project", "–≤—ã–ø–æ–ª–Ω–∏–ª"]
        ):
            return "expert_projects"
        elif any(
            word in question_lower
            for word in [
                "–æ–ø—ã—Ç",
                "experience",
                "—Å—Ç–∞–≤–∫",
                "rate",
                "–Ω–∞–≤—ã–∫",
                "skill",
                "—á–∞—Å–æ–≤",
            ]
        ):
            return "experience_rates"
        elif any(
            word in question_lower
            for word in [
                "—Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü",
                "specialization",
                "–∫–∞—Ç–µ–≥–æ—Ä",
                "category",
                "–ø—Ä–∏–±—ã–ª—å–Ω",
            ]
        ):
            return "specialization_earnings"
        elif any(
            word in question_lower
            for word in [
                "–ø–ª–∞—Ç—Ñ–æ—Ä–º",
                "platform",
                "fiverr",
                "upwork",
                "freelancer",
                "—Ç–æ–ø—Ç–∞–ª",
            ]
        ):
            return "platform_performance"
        else:
            return "summary"

    def _generate_context_prompt(
        self, analysis_type: str, analysis_data: Dict[str, Any], user_question: str
    ) -> str:
        """
        Generate a context-rich prompt for Claude.

        Args:
            analysis_type: Type of analysis performed
            analysis_data: Results from data analysis
            user_question: Original user question

        Returns:
            Formatted prompt for Claude
        """
        # Convert analysis data to readable format
        data_summary = format_data_as_json(analysis_data, indent=2)

        context_prompt = f"""
–¢—ã - —ç–∫—Å–ø–µ—Ä—Ç –∞–Ω–∞–ª–∏—Ç–∏–∫ –¥–∞–Ω–Ω—ã—Ö —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤. –û—Ç–≤–µ—Ç—å –Ω–∞ –≤–æ–ø—Ä–æ—Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è, –∏—Å–ø–æ–ª—å–∑—É—è –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ.

–í–û–ü–†–û–° –ü–û–õ–¨–ó–û–í–ê–¢–ï–õ–Ø: {user_question}

–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ê–ù–ê–õ–ò–ó–ê –î–ê–ù–ù–´–•:
{data_summary}

–ò–ù–°–¢–†–£–ö–¶–ò–ò:
1. –î–∞–π —á–µ—Ç–∫–∏–π –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
2. –ò—Å–ø–æ–ª—å–∑—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–µ —á–∏—Å–ª–∞ –∏ –ø—Ä–æ—Ü–µ–Ω—Ç—ã –∏–∑ –∞–Ω–∞–ª–∏–∑–∞
3. –û–±—ä—è—Å–Ω–∏ –æ—Å–Ω–æ–≤–Ω—ã–µ —Ç—Ä–µ–Ω–¥—ã –∏ –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–∏
4. –°–¥–µ–ª–∞–π –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏–µ –≤—ã–≤–æ–¥—ã –¥–ª—è —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤
5. –ò—Å–ø–æ–ª—å–∑—É–π –ø—Ä–æ—Å—Ç–æ–π –∏ –ø–æ–Ω—è—Ç–Ω—ã–π —è–∑—ã–∫
6. –§–æ—Ä–º–∞—Ç–∏—Ä—É–π –æ—Ç–≤–µ—Ç —Å –∑–∞–≥–æ–ª–æ–≤–∫–∞–º–∏ –∏ —Å–ø–∏—Å–∫–∞–º–∏ –¥–ª—è –ª—É—á—à–µ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏

–û–¢–í–ï–¢:"""
        return context_prompt

    def query_llm(self, prompt: str) -> str:
        """
        Query Claude through Anthropic API.

        Args:
            prompt: Formatted prompt for Claude

        Returns:
            Claude response text
        """
        if not self.client:
            return self._generate_fallback_response()

        try:
            print("ü§ñ Querying Claude...")

            # Get configuration from environment variables with defaults
            model = os.getenv("CLAUDE_MODEL", "claude-3-5-sonnet-20241022")
            temperature = float(os.getenv("CLAUDE_TEMPERATURE", "0.1"))
            max_tokens = int(os.getenv("CLAUDE_MAX_TOKENS", "4000"))

            message = self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}],
            )

            # Handle different content types from Claude
            content = message.content[0]
            if hasattr(content, "text"):
                return str(content.text)
            else:
                return str(content)

        except Exception as e:
            print(f"‚ùå Claude query failed: {e}")
            return self._generate_fallback_response()

    def _generate_fallback_response(self) -> str:
        """
        Generate a fallback response when Claude is not available.

        Returns:
            Fallback response text
        """
        return """
ü§ñ –ê–Ω–∞–ª–∏–∑ –¥–∞–Ω–Ω—ã—Ö –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ!

–ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, Claude LLM —Å–µ—Ä–≤–∏—Å –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω, –Ω–æ –¥–∞–Ω–Ω—ã–µ –±—ã–ª–∏ –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã.

üìä **–ß—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å Claude –æ—Ç–≤–µ—Ç—ã:**

1. **–ü–æ–ª—É—á–∏—Ç–µ Anthropic API –∫–ª—é—á:**
   - –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Ç–µ—Å—å –Ω–∞ https://console.anthropic.com/
   - –°–æ–∑–¥–∞–π—Ç–µ API –∫–ª—é—á –≤ —Ä–∞–∑–¥–µ–ª–µ API Keys
   - –°—Ç–æ–∏–º–æ—Å—Ç—å: ~$0.003 –∑–∞ 1000 —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è Claude-3.5-Sonnet

2. **–£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è:**
   ```bash
   export ANTHROPIC_API_KEY="your-api-key-here"
   ```

3. **–ò–ª–∏ –¥–æ–±–∞–≤—å—Ç–µ –≤ .env —Ñ–∞–π–ª:**
   ```bash
   ANTHROPIC_API_KEY=your-api-key-here
   ```

4. **–ü–µ—Ä–µ–∑–∞–ø—É—Å—Ç–∏—Ç–µ —Å–∏—Å—Ç–µ–º—É:**
   ```bash
   uv run -m src.main ask "–≤–∞—à –≤–æ–ø—Ä–æ—Å"
   ```

üí° **–°–æ–≤–µ—Ç:** –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫–æ–º–∞–Ω–¥—É 'analyze' –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –¥–µ—Ç–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∞–Ω–∞–ª–∏–∑–∞ –±–µ–∑ LLM.

üìà **–û—Å–Ω–æ–≤–Ω—ã–µ –≤—ã–≤–æ–¥—ã –∏–∑ –∞–Ω–∞–ª–∏–∑–∞:**
- –ö—Ä–∏–ø—Ç–æ–ø–ª–∞—Ç–µ–∂–∏ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç —Ä–∞–∑–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Ä–µ–≥–∏–æ–Ω–∞–º
- –≠–∫—Å–ø–µ—Ä—Ç–Ω–æ—Å—Ç—å –≤–ª–∏—è–µ—Ç –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–æ–µ–∫—Ç–æ–≤ –∏ –¥–æ—Ö–æ–¥—ã
- –°–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∏–≥—Ä–∞–µ—Ç –∫–ª—é—á–µ–≤—É—é —Ä–æ–ª—å –≤ –¥–æ—Ö–æ–¥–∞—Ö
"""

    def process_question(self, question: str) -> Dict[str, Any]:
        """
        Process a natural language question and return comprehensive response.

        Args:
            question: User's question in natural language

        Returns:
            Dictionary with analysis results and Claude response
        """
        try:
            # Step 1: Classify the question
            analysis_type = self.classify_question(question)
            print(f"üîç Detected question type: {analysis_type}")

            # Step 2: Run appropriate analysis
            analysis_function = self.question_mapping[analysis_type]
            analysis_data = analysis_function()
            print("üìä Analysis completed")

            # Step 3: Generate context prompt
            context_prompt = self._generate_context_prompt(
                analysis_type, analysis_data, question
            )

            # Step 4: Get Claude response
            llm_response = self.query_llm(context_prompt)

            return {
                "question": question,
                "analysis_type": analysis_type,
                "analysis_data": analysis_data,
                "llm_response": llm_response,
                "status": "success",
            }

        except Exception as e:
            print(f"‚ùå Error processing question: {e}")
            return {
                "question": question,
                "error": str(e),
                "status": "error",
                "fallback_response": self._generate_fallback_response(),
            }

    def get_sample_questions(self) -> List[str]:
        """
        Get list of sample questions that the system can answer.

        Returns:
            List of sample questions
        """
        return [
            "–ù–∞—Å–∫–æ–ª—å–∫–æ –≤—ã—à–µ –¥–æ—Ö–æ–¥ —É —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤, –ø—Ä–∏–Ω–∏–º–∞—é—â–∏—Ö –æ–ø–ª–∞—Ç—É –≤ –∫—Ä–∏–ø—Ç–æ–≤–∞–ª—é—Ç–µ, –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å –¥—Ä—É–≥–∏–º–∏ —Å–ø–æ—Å–æ–±–∞–º–∏ –æ–ø–ª–∞—Ç—ã?",
            "–ö–∞–∫ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª—è–µ—Ç—Å—è –¥–æ—Ö–æ–¥ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ä–µ–≥–∏–æ–Ω–∞ –ø—Ä–æ–∂–∏–≤–∞–Ω–∏—è?",
            "–ö–∞–∫–æ–π –ø—Ä–æ—Ü–µ–Ω—Ç —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤, —Å—á–∏—Ç–∞—é—â–∏—Ö —Å–µ–±—è —ç–∫—Å–ø–µ—Ä—Ç–∞–º–∏, –≤—ã–ø–æ–ª–Ω–∏–ª –º–µ–Ω–µ–µ 100 –ø—Ä–æ–µ–∫—Ç–æ–≤?",
            "–ö–∞–∫ —Å–≤—è–∑–∞–Ω —É—Ä–æ–≤–µ–Ω—å –æ–ø—ã—Ç–∞ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–∞ —Å –µ–≥–æ —á–∞—Å–æ–≤–æ–π —Å—Ç–∞–≤–∫–æ–π?",
            "–ö–∞–∫–∏–µ —Å–ø–µ—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤ –Ω–∞–∏–±–æ–ª–µ–µ –ø—Ä–∏–±—ã–ª—å–Ω—ã?",
            "–ù–∞ –∫–∞–∫–æ–π –ø–ª–∞—Ç—Ñ–æ—Ä–º–µ —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä—ã –∑–∞—Ä–∞–±–∞—Ç—ã–≤–∞—é—Ç –±–æ–ª—å—à–µ –≤—Å–µ–≥–æ?",
            "–î–∞–π—Ç–µ –æ–±—â—É—é —Å–≤–æ–¥–∫—É –ø–æ —Ä—ã–Ω–∫—É —Ñ—Ä–∏–ª–∞–Ω—Å–µ—Ä–æ–≤",
        ]

    def health_check(self) -> Dict[str, Any]:
        """
        Check the health of Claude service.

        Returns:
            Health status information
        """
        health_status = {
            "data_analyzer": True,
            "anthropic_available": ANTHROPIC_AVAILABLE,
            "anthropic_api_key_set": bool(os.getenv("ANTHROPIC_API_KEY")),
            "llm_initialized": self.client is not None,
            "overall_status": "healthy" if self.client else "llm_unavailable",
        }

        # Test Claude if available
        if self.client:
            try:
                test_response = self.query_llm("Hello")
                health_status["llm_test"] = "passed" if test_response else "failed"
            except Exception as e:
                health_status["llm_test"] = f"failed: {str(e)}"
                health_status["overall_status"] = "degraded"
        else:
            health_status["llm_test"] = "not_available"

        return health_status
