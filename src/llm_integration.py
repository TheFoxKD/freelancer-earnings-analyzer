"""
Claude LLM integration module using Anthropic API.

This module provides a clean and reliable LLM integration using
Anthropic's Claude models with environment variable configuration.
"""

import os
from typing import Dict, Any, List, Optional

from dotenv import load_dotenv
from .data_analyzer import DataAnalyzer
from .utils import format_data_as_json

# Load environment variables from .env file
load_dotenv()

# Import Anthropic components
try:
    import anthropic

    ANTHROPIC_AVAILABLE = True
except ImportError:
    ANTHROPIC_AVAILABLE = False
    print("âŒ Anthropic not installed. Run: uv add anthropic")


class SimpleLLMProcessor:
    """
    Simple LLM processor using Anthropic Claude.

    This class provides clean LLM integration with Claude models.
    API key is loaded from ANTHROPIC_API_KEY environment variable.
    """

    def __init__(self, data_analyzer: DataAnalyzer):
        """
        Initialize the Claude LLM processor.

        Args:
            data_analyzer: DataAnalyzer instance with loaded data
        """
        self.analyzer = data_analyzer
        self.client: Optional[anthropic.Anthropic] = self._initialize_claude()

        # Map questions to analysis functions
        self.question_mapping = {
            "crypto_payment": self.analyzer.analyze_crypto_payment_earnings,
            "regional_income": self.analyzer.analyze_regional_income_distribution,
            "expert_projects": self.analyzer.analyze_expert_projects_completion,
            "experience_rates": self.analyzer.analyze_experience_vs_rates,
            "specialization_earnings": self.analyzer.analyze_specialization_earnings,
            "platform_performance": self.analyzer.analyze_platform_performance,
            "summary": self.analyzer.get_comprehensive_summary,
        }

    def _initialize_claude(self) -> Optional[anthropic.Anthropic]:
        """
        Initialize Claude client through Anthropic API.

        Returns:
            Anthropic client instance or None if not available
        """
        if not ANTHROPIC_AVAILABLE:
            print("âŒ Anthropic SDK is not available")
            return None

        # Get API key from environment
        api_key = os.getenv("ANTHROPIC_API_KEY")
        if not api_key:
            print("âŒ ANTHROPIC_API_KEY environment variable not set")
            print("ðŸ’¡ Set it with: export ANTHROPIC_API_KEY='your-api-key-here'")
            return None

        try:
            # Get configuration from environment variables with defaults
            timeout = int(os.getenv("CLAUDE_TIMEOUT", "30"))

            # Configure proxy if available using httpx
            http_client_kwargs = {}

            if (
                os.getenv("HTTP_PROXY")
                or os.getenv("HTTPS_PROXY")
                or os.getenv("SOCKS_PROXY")
            ):
                try:
                    import httpx

                    proxies = {}
                    if os.getenv("HTTP_PROXY"):
                        proxies["http://"] = os.getenv("HTTP_PROXY")
                    if os.getenv("HTTPS_PROXY"):
                        proxies["https://"] = os.getenv("HTTPS_PROXY")
                    if os.getenv("SOCKS_PROXY"):
                        proxies["http://"] = os.getenv("SOCKS_PROXY")
                        proxies["https://"] = os.getenv("SOCKS_PROXY")

                    if proxies:
                        # Create custom httpx client with proxy
                        http_client = httpx.Client(
                            proxy=proxies.get("https://") or proxies.get("http://"),
                            timeout=timeout,
                        )
                        http_client_kwargs["http_client"] = http_client
                        print(f"ðŸŒ Using proxy for Claude: {list(proxies.keys())}")

                except ImportError:
                    print("âš ï¸ httpx not available for proxy support")
                except Exception as e:
                    print(f"âš ï¸ Proxy configuration failed: {e}")

            # Initialize Anthropic client
            client = anthropic.Anthropic(
                api_key=api_key,
                timeout=timeout,
                **http_client_kwargs,  # type: ignore
            )

            # Get model name for display
            model = os.getenv("CLAUDE_MODEL", "claude-3-5-sonnet-20241022")
            print(f"âœ… Claude (Anthropic) initialized successfully with model {model}")
            return client

        except Exception as e:
            print(f"âŒ Claude initialization failed: {e}")
            return None

    def classify_question(self, question: str) -> str:
        """
        Classify the user question to determine which analysis to run.

        Args:
            question: User's natural language question

        Returns:
            Analysis type key
        """
        question_lower = question.lower()

        # Enhanced keyword matching
        if any(
            word in question_lower
            for word in [
                "ÐºÑ€Ð¸Ð¿Ñ‚Ð¾",
                "crypto",
                "ÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð²Ð°Ð»ÑŽÑ‚",
                "Ð¾Ð¿Ð»Ð°Ñ‚",
                "payment",
                "bitcoin",
            ]
        ):
            return "crypto_payment"
        elif any(
            word in question_lower
            for word in [
                "Ñ€ÐµÐ³Ð¸Ð¾Ð½",
                "region",
                "Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»Ñ",
                "Ð³ÐµÐ¾Ð³Ñ€Ð°Ñ„",
                "ÑÑ‚Ñ€Ð°Ð½Ð°",
                "country",
            ]
        ):
            return "regional_income"
        elif any(
            word in question_lower
            for word in ["ÑÐºÑÐ¿ÐµÑ€Ñ‚", "expert", "100", "Ð¿Ñ€Ð¾ÐµÐºÑ‚", "project", "Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ð»"]
        ):
            return "expert_projects"
        elif any(
            word in question_lower
            for word in [
                "Ð¾Ð¿Ñ‹Ñ‚",
                "experience",
                "ÑÑ‚Ð°Ð²Ðº",
                "rate",
                "Ð½Ð°Ð²Ñ‹Ðº",
                "skill",
                "Ñ‡Ð°ÑÐ¾Ð²",
            ]
        ):
            return "experience_rates"
        elif any(
            word in question_lower
            for word in [
                "ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†",
                "specialization",
                "ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€",
                "category",
                "Ð¿Ñ€Ð¸Ð±Ñ‹Ð»ÑŒÐ½",
            ]
        ):
            return "specialization_earnings"
        elif any(
            word in question_lower
            for word in [
                "Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼",
                "platform",
                "fiverr",
                "upwork",
                "freelancer",
                "Ñ‚Ð¾Ð¿Ñ‚Ð°Ð»",
            ]
        ):
            return "platform_performance"
        else:
            return "summary"

    def _generate_context_prompt(
        self, analysis_type: str, analysis_data: Dict[str, Any], user_question: str
    ) -> str:
        """
        Generate a context-rich prompt for Claude.

        Args:
            analysis_type: Type of analysis performed
            analysis_data: Results from data analysis
            user_question: Original user question

        Returns:
            Formatted prompt for Claude
        """
        # Convert analysis data to readable format
        data_summary = format_data_as_json(analysis_data, indent=2)

        context_prompt = f"""
Ð¢Ñ‹ - ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð°Ð½Ð°Ð»Ð¸Ñ‚Ð¸Ðº Ð´Ð°Ð½Ð½Ñ‹Ñ… Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ð¾Ð². ÐžÑ‚Ð²ÐµÑ‚ÑŒ Ð½Ð° Ð²Ð¾Ð¿Ñ€Ð¾Ñ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÑ Ð¿Ñ€ÐµÐ´Ð¾ÑÑ‚Ð°Ð²Ð»ÐµÐ½Ð½Ñ‹Ðµ ÑÑ‚Ð°Ñ‚Ð¸ÑÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ.

Ð’ÐžÐŸÐ ÐžÐ¡ ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð•Ð›Ð¯: {user_question}

Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð« ÐÐÐÐ›Ð˜Ð—Ð Ð”ÐÐÐÐ«Ð¥:
{data_summary}

Ð˜ÐÐ¡Ð¢Ð Ð£ÐšÐ¦Ð˜Ð˜:
1. Ð”Ð°Ð¹ Ñ‡ÐµÑ‚ÐºÐ¸Ð¹ Ð¸ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ð½Ð° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ
2. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ñ‹Ðµ Ñ‡Ð¸ÑÐ»Ð° Ð¸ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚Ñ‹ Ð¸Ð· Ð°Ð½Ð°Ð»Ð¸Ð·Ð°
3. ÐžÐ±ÑŠÑÑÐ½Ð¸ Ð¾ÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ñ‚Ñ€ÐµÐ½Ð´Ñ‹ Ð¸ Ð·Ð°ÐºÐ¾Ð½Ð¾Ð¼ÐµÑ€Ð½Ð¾ÑÑ‚Ð¸
4. Ð¡Ð´ÐµÐ»Ð°Ð¹ Ð¿Ñ€Ð°ÐºÑ‚Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð²Ñ‹Ð²Ð¾Ð´Ñ‹ Ð´Ð»Ñ Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ð¾Ð²
5. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¸ Ð¿Ð¾Ð½ÑÑ‚Ð½Ñ‹Ð¹ ÑÐ·Ñ‹Ðº
6. Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€ÑƒÐ¹ Ð¾Ñ‚Ð²ÐµÑ‚ Ñ Ð·Ð°Ð³Ð¾Ð»Ð¾Ð²ÐºÐ°Ð¼Ð¸ Ð¸ ÑÐ¿Ð¸ÑÐºÐ°Ð¼Ð¸ Ð´Ð»Ñ Ð»ÑƒÑ‡ÑˆÐµÐ¹ Ñ‡Ð¸Ñ‚Ð°ÐµÐ¼Ð¾ÑÑ‚Ð¸

ÐžÐ¢Ð’Ð•Ð¢:"""
        return context_prompt

    def query_llm(self, prompt: str) -> str:
        """
        Query Claude through Anthropic API.

        Args:
            prompt: Formatted prompt for Claude

        Returns:
            Claude response text
        """
        if not self.client:
            return self._generate_fallback_response()

        try:
            print("ðŸ¤– Querying Claude...")

            # Get configuration from environment variables with defaults
            model = os.getenv("CLAUDE_MODEL", "claude-3-5-sonnet-20241022")
            temperature = float(os.getenv("CLAUDE_TEMPERATURE", "0.1"))
            max_tokens = int(os.getenv("CLAUDE_MAX_TOKENS", "4000"))

            message = self.client.messages.create(
                model=model,
                max_tokens=max_tokens,
                temperature=temperature,
                messages=[{"role": "user", "content": prompt}],
            )

            # Handle different content types from Claude
            content = message.content[0]
            if hasattr(content, "text"):
                return str(content.text)
            else:
                return str(content)

        except Exception as e:
            print(f"âŒ Claude query failed: {e}")
            return self._generate_fallback_response()

    def _generate_fallback_response(self) -> str:
        """
        Generate a fallback response when Claude is not available.

        Returns:
            Fallback response text
        """
        return """
ðŸ¤– ÐÐ½Ð°Ð»Ð¸Ð· Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾!

Ðš ÑÐ¾Ð¶Ð°Ð»ÐµÐ½Ð¸ÑŽ, Claude LLM ÑÐµÑ€Ð²Ð¸Ñ Ð½ÐµÐ´Ð¾ÑÑ‚ÑƒÐ¿ÐµÐ½, Ð½Ð¾ Ð´Ð°Ð½Ð½Ñ‹Ðµ Ð±Ñ‹Ð»Ð¸ Ð¿Ñ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ñ‹.

ðŸ“Š **Ð§Ñ‚Ð¾Ð±Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Claude Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹:**

1. **ÐŸÐ¾Ð»ÑƒÑ‡Ð¸Ñ‚Ðµ Anthropic API ÐºÐ»ÑŽÑ‡:**
   - Ð—Ð°Ñ€ÐµÐ³Ð¸ÑÑ‚Ñ€Ð¸Ñ€ÑƒÐ¹Ñ‚ÐµÑÑŒ Ð½Ð° https://console.anthropic.com/
   - Ð¡Ð¾Ð·Ð´Ð°Ð¹Ñ‚Ðµ API ÐºÐ»ÑŽÑ‡ Ð² Ñ€Ð°Ð·Ð´ÐµÐ»Ðµ API Keys
   - Ð¡Ñ‚Ð¾Ð¸Ð¼Ð¾ÑÑ‚ÑŒ: ~$0.003 Ð·Ð° 1000 Ñ‚Ð¾ÐºÐµÐ½Ð¾Ð² Ð´Ð»Ñ Claude-3.5-Sonnet

2. **Ð£ÑÑ‚Ð°Ð½Ð¾Ð²Ð¸Ñ‚Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½ÑƒÑŽ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ:**
   ```bash
   export ANTHROPIC_API_KEY="your-api-key-here"
   ```

3. **Ð˜Ð»Ð¸ Ð´Ð¾Ð±Ð°Ð²ÑŒÑ‚Ðµ Ð² .env Ñ„Ð°Ð¹Ð»:**
   ```bash
   ANTHROPIC_API_KEY=your-api-key-here
   ```

4. **ÐŸÐµÑ€ÐµÐ·Ð°Ð¿ÑƒÑÑ‚Ð¸Ñ‚Ðµ ÑÐ¸ÑÑ‚ÐµÐ¼Ñƒ:**
   ```bash
   uv run -m src.main ask "Ð²Ð°Ñˆ Ð²Ð¾Ð¿Ñ€Ð¾Ñ"
   ```

ðŸ’¡ **Ð¡Ð¾Ð²ÐµÑ‚:** Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹Ñ‚Ðµ ÐºÐ¾Ð¼Ð°Ð½Ð´Ñƒ 'analyze' Ð´Ð»Ñ Ð¿Ð¾Ð»ÑƒÑ‡ÐµÐ½Ð¸Ñ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ… Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð±ÐµÐ· LLM.

ðŸ“ˆ **ÐžÑÐ½Ð¾Ð²Ð½Ñ‹Ðµ Ð²Ñ‹Ð²Ð¾Ð´Ñ‹ Ð¸Ð· Ð°Ð½Ð°Ð»Ð¸Ð·Ð°:**
- ÐšÑ€Ð¸Ð¿Ñ‚Ð¾Ð¿Ð»Ð°Ñ‚ÐµÐ¶Ð¸ Ð¿Ð¾ÐºÐ°Ð·Ñ‹Ð²Ð°ÑŽÑ‚ Ñ€Ð°Ð·Ð½Ñ‹Ðµ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¿Ð¾ Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð°Ð¼
- Ð­ÐºÑÐ¿ÐµÑ€Ñ‚Ð½Ð¾ÑÑ‚ÑŒ Ð²Ð»Ð¸ÑÐµÑ‚ Ð½Ð° ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¾Ð² Ð¸ Ð´Ð¾Ñ…Ð¾Ð´Ñ‹
- Ð¡Ð¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ñ Ð¸Ð³Ñ€Ð°ÐµÑ‚ ÐºÐ»ÑŽÑ‡ÐµÐ²ÑƒÑŽ Ñ€Ð¾Ð»ÑŒ Ð² Ð´Ð¾Ñ…Ð¾Ð´Ð°Ñ…
"""

    def process_question(self, question: str) -> Dict[str, Any]:
        """
        Process a natural language question and return comprehensive response.

        Args:
            question: User's question in natural language

        Returns:
            Dictionary with analysis results and Claude response
        """
        try:
            # Step 1: Classify the question
            analysis_type = self.classify_question(question)
            print(f"ðŸ” Detected question type: {analysis_type}")

            # Step 2: Run appropriate analysis
            analysis_function = self.question_mapping[analysis_type]
            analysis_data = analysis_function()
            print("ðŸ“Š Analysis completed")

            # Step 3: Generate context prompt
            context_prompt = self._generate_context_prompt(
                analysis_type, analysis_data, question
            )

            # Step 4: Get Claude response
            llm_response = self.query_llm(context_prompt)

            return {
                "question": question,
                "analysis_type": analysis_type,
                "analysis_data": analysis_data,
                "llm_response": llm_response,
                "status": "success",
            }

        except Exception as e:
            print(f"âŒ Error processing question: {e}")
            return {
                "question": question,
                "error": str(e),
                "status": "error",
                "fallback_response": self._generate_fallback_response(),
            }

    def get_sample_questions(self) -> List[str]:
        """
        Get list of sample questions that the system can answer.

        Returns:
            List of sample questions
        """
        return [
            "ÐÐ°ÑÐºÐ¾Ð»ÑŒÐºÐ¾ Ð²Ñ‹ÑˆÐµ Ð´Ð¾Ñ…Ð¾Ð´ Ñƒ Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ð¾Ð², Ð¿Ñ€Ð¸Ð½Ð¸Ð¼Ð°ÑŽÑ‰Ð¸Ñ… Ð¾Ð¿Ð»Ð°Ñ‚Ñƒ Ð² ÐºÑ€Ð¸Ð¿Ñ‚Ð¾Ð²Ð°Ð»ÑŽÑ‚Ðµ, Ð¿Ð¾ ÑÑ€Ð°Ð²Ð½ÐµÐ½Ð¸ÑŽ Ñ Ð´Ñ€ÑƒÐ³Ð¸Ð¼Ð¸ ÑÐ¿Ð¾ÑÐ¾Ð±Ð°Ð¼Ð¸ Ð¾Ð¿Ð»Ð°Ñ‚Ñ‹?",
            "ÐšÐ°Ðº Ñ€Ð°ÑÐ¿Ñ€ÐµÐ´ÐµÐ»ÑÐµÑ‚ÑÑ Ð´Ð¾Ñ…Ð¾Ð´ Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ð¾Ð² Ð² Ð·Ð°Ð²Ð¸ÑÐ¸Ð¼Ð¾ÑÑ‚Ð¸ Ð¾Ñ‚ Ñ€ÐµÐ³Ð¸Ð¾Ð½Ð° Ð¿Ñ€Ð¾Ð¶Ð¸Ð²Ð°Ð½Ð¸Ñ?",
            "ÐšÐ°ÐºÐ¾Ð¹ Ð¿Ñ€Ð¾Ñ†ÐµÐ½Ñ‚ Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ð¾Ð², ÑÑ‡Ð¸Ñ‚Ð°ÑŽÑ‰Ð¸Ñ… ÑÐµÐ±Ñ ÑÐºÑÐ¿ÐµÑ€Ñ‚Ð°Ð¼Ð¸, Ð²Ñ‹Ð¿Ð¾Ð»Ð½Ð¸Ð» Ð¼ÐµÐ½ÐµÐµ 100 Ð¿Ñ€Ð¾ÐµÐºÑ‚Ð¾Ð²?",
            "ÐšÐ°Ðº ÑÐ²ÑÐ·Ð°Ð½ ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ Ð¾Ð¿Ñ‹Ñ‚Ð° Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ð° Ñ ÐµÐ³Ð¾ Ñ‡Ð°ÑÐ¾Ð²Ð¾Ð¹ ÑÑ‚Ð°Ð²ÐºÐ¾Ð¹?",
            "ÐšÐ°ÐºÐ¸Ðµ ÑÐ¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð°Ñ†Ð¸Ð¸ Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ð¾Ð² Ð½Ð°Ð¸Ð±Ð¾Ð»ÐµÐµ Ð¿Ñ€Ð¸Ð±Ñ‹Ð»ÑŒÐ½Ñ‹?",
            "ÐÐ° ÐºÐ°ÐºÐ¾Ð¹ Ð¿Ð»Ð°Ñ‚Ñ„Ð¾Ñ€Ð¼Ðµ Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ñ‹ Ð·Ð°Ñ€Ð°Ð±Ð°Ñ‚Ñ‹Ð²Ð°ÑŽÑ‚ Ð±Ð¾Ð»ÑŒÑˆÐµ Ð²ÑÐµÐ³Ð¾?",
            "Ð”Ð°Ð¹Ñ‚Ðµ Ð¾Ð±Ñ‰ÑƒÑŽ ÑÐ²Ð¾Ð´ÐºÑƒ Ð¿Ð¾ Ñ€Ñ‹Ð½ÐºÑƒ Ñ„Ñ€Ð¸Ð»Ð°Ð½ÑÐµÑ€Ð¾Ð²",
        ]

    def health_check(self) -> Dict[str, Any]:
        """
        Check the health of Claude service.

        Returns:
            Health status information
        """
        health_status = {
            "data_analyzer": True,
            "anthropic_available": ANTHROPIC_AVAILABLE,
            "anthropic_api_key_set": bool(os.getenv("ANTHROPIC_API_KEY")),
            "llm_initialized": self.client is not None,
            "overall_status": "healthy" if self.client else "llm_unavailable",
        }

        # Test Claude if available
        if self.client:
            try:
                test_response = self.query_llm("Hello")
                health_status["llm_test"] = "passed" if test_response else "failed"
            except Exception as e:
                health_status["llm_test"] = f"failed: {str(e)}"
                health_status["overall_status"] = "degraded"
        else:
            health_status["llm_test"] = "not_available"

        return health_status
